# Import libraries
import torch
import time
from environment import PruningEnv
from REINFORCE_agent import REINFORCE_agent
import os
import logging
import numpy as np
logging.basicConfig(level=logging.INFO, 
                    format=('%(levelname)s:' +
                            '[%(filename)s:%(lineno)d]' +
                            ' %(message)s'))

# Define Agent, Training Env, & Hyper-params
env = PruningEnv()
print(env.state_size)
agent = REINFORCE_agent(env.state_size, 512)

M = 1# no reason, number of training episodes
layers_to_prune = [] # will be list of string names
for layer_name, _ in env.model.named_modules():
    if "conv" in layer_name:
        layers_to_prune.append(layer_name)

#Initialize empty lists.
listacc = np.zeros([4,M])
listnumpruned = np.zeros([4,M])
listreward = np.zeros([4,M])
listflop = np.zeros([4,M])
listvalacc = np.zeros([100])

for episode in range(M):
    print("=====",episode,"New Episode======================================")
    env.reset_to_k() # reset CNN to full-params
    #env._evaluate_model()
    action_reward_buffer = [] # list of (action,reward) tuples per episode

    # single rollout, layer-by-layer CNN scan
    for layer_name in layers_to_prune:
        env.layer_to_process = layer_name
        print("===== Working on", layer_name, "layer =====")
        # get state from orig model (or should we get from pruned model?)
        state = env.get_state()
        
        action = agent.get_action(state)
        action = torch.unsqueeze(action, 0)
        #logging.info("Actions: {}".format(action))
        action_to_index = (action > 0.5).type(torch.int)
        #logging.info("action_to_index sum: {}".format(action_to_index.sum()))
        env.prune_layer(action_to_index)
        print("Calculating reward")
        total_filters = 64*(2**(int(layer_name[-1])-1))
        amount_pruned = action_to_index[0,\
                                0:64*(2**(int(layer_name[-1])-1))].sum()
        reward,acc,flop = env._calculate_reward(amount_pruned,total_filters)
        
        #reward = reward * -1
        #it seems this doesnt matter? as long as it heads to 0
        #        then that is the goal maybe this is a lr problem
        action_reward_buffer.append((action, reward))

        
        # print("For",layer_name,"pruned",action_to_index[0,\
        #                    0:64*(2**(int(layer_name[-1])-1))].sum())
        # print("FILT",64*(2**(int(layer_name[-1])-1)))
        
        #Log info to be printed as needed
        listacc[int(layer_name[-1])-1, episode] = acc
        listnumpruned[int(layer_name[-1])-1, episode] = \
                    action_to_index[0,0:64*(2**(int(layer_name[-1])-1))].sum()
        listreward[int(layer_name[-1])-1, episode] = reward
        listflop[int(layer_name[-1])-1, episode] = flop
        
    # calc cumulative reward, agent learns 
    actions, rewards = zip(*action_reward_buffer) # both var are tuple wrapped
    # actions: tuple->tensor
    actions = torch.squeeze(torch.stack(actions)).type(torch.float)
    agent.update_policy(rewards, actions) 

raise KeyboardInterrupt # break 

#PATH = os.getcwd() + '\pruned_50_1_train_after.pth'
#torch.save(env.model.state_dict(), PATH)
for i in range(0):
    env._train_model(num_epochs = 1)
    listvalacc[i] = env._evaluate_model()
print("List of accuraccies",listacc)
print("list of pruned", listnumpruned)
print("list of rewards", listreward)
env._evaluate_model







#PATH = os.getcwd() + '\pruned_50_1_train_after_after.pth'
#torch.save(env.model.state_dict(), PATH)
# List of accuraccies [[0.2609 0.3069 0.3071 0.2882 0.3496 0.3496 0.3425 0.3425 0.3425 0.3425
  # 0.3425 0.3425 0.3288 0.3288 0.3423 0.3423 0.3423 0.3423 0.3423 0.3423
  # 0.3423 0.3423 0.3423 0.3423 0.3423 0.3423 0.3423 0.3423 0.3423 0.3423
  # 0.3423 0.3423 0.3111 0.2968 0.2968 0.2968 0.2968 0.2968 0.2968 0.2968
  # 0.2968 0.2968 0.2968 0.2968 0.2968 0.2924 0.2924 0.2924 0.2924 0.2924]
 # [0.1106 0.1194 0.1338 0.127  0.1712 0.1606 0.1654 0.1645 0.1598 0.1598
  # 0.1636 0.1636 0.1596 0.1607 0.173  0.175  0.1847 0.1847 0.1757 0.1757
  # 0.1757 0.1757 0.1757 0.1757 0.1757 0.1757 0.1757 0.1757 0.1757 0.1757
  # 0.1757 0.1757 0.1641 0.1567 0.1567 0.1567 0.1582 0.1582 0.1582 0.1582
  # 0.1582 0.1531 0.1531 0.1531 0.1569 0.1564 0.1564 0.1564 0.1564 0.1564]
 # [0.1039 0.1139 0.1214 0.1214 0.1691 0.1615 0.1551 0.1493 0.1481 0.1532
  # 0.1556 0.1585 0.158  0.1729 0.1851 0.1932 0.2011 0.2043 0.2046 0.1997
  # 0.2024 0.2012 0.2014 0.2009 0.1936 0.1935 0.1926 0.1882 0.1882 0.1882
  # 0.1963 0.1979 0.1818 0.1743 0.1745 0.1737 0.1757 0.1761 0.1761 0.1758
  # 0.1752 0.1712 0.1712 0.1622 0.1663 0.1647 0.1648 0.1633 0.172  0.171 ]
 # [0.1037 0.1129 0.1176 0.1146 0.1658 0.1509 0.1382 0.1356 0.1362 0.1371
  # 0.1417 0.1424 0.1443 0.1583 0.1657 0.1729 0.1755 0.1829 0.1866 0.1826
  # 0.1787 0.1797 0.1807 0.1778 0.176  0.1724 0.1669 0.1615 0.1668 0.1677
  # 0.1786 0.1796 0.163  0.1549 0.1533 0.153  0.1498 0.1509 0.1534 0.1543
  # 0.1542 0.1532 0.1516 0.1448 0.1465 0.1446 0.1436 0.1411 0.1521 0.1543]]
# list of pruned [[ 30.  30.  30.  29.  32.  32.  31.  31.  31.  31.  31.  31.  30.  30.
   # 31.  31.  31.  31.  31.  31.  31.  31.  31.  31.  31.  31.  31.  31.
   # 31.  31.  31.  31.  29.  28.  28.  28.  28.  28.  28.  28.  28.  28.
   # 28.  28.  28.  27.  27.  27.  27.  27.]
 # [ 61.  62.  69.  66.  73.  69.  71.  70.  69.  69.  70.  70.  70.  71.
   # 71.  72.  74.  74.  73.  73.  73.  73.  73.  73.  73.  73.  73.  73.
   # 73.  73.  73.  73.  72.  72.  72.  72.  71.  71.  71.  71.  71.  69.
   # 69.  69.  70.  70.  70.  70.  70.  70.]
 # [131. 130. 126. 124. 128. 128. 130. 135. 137. 137. 137. 134. 138. 141.
  # 144. 142. 144. 146. 148. 148. 148. 149. 150. 149. 149. 148. 148. 147.
  # 147. 147. 148. 147. 148. 148. 147. 147. 147. 146. 147. 146. 144. 143.
  # 143. 144. 142. 143. 144. 145. 147. 146.]
 # [247. 251. 255. 252. 249. 259. 262. 269. 267. 260. 262. 266. 269. 270.
  # 268. 271. 271. 278. 277. 280. 278. 281. 283. 283. 280. 276. 276. 276.
  # 279. 276. 277. 275. 278. 276. 276. 278. 277. 269. 267. 270. 271. 273.
  # 274. 271. 272. 273. 276. 276. 278. 273.]]
  
  
# List of accuraccies [[0.4824 0.4578 0.4706 0.485  0.4815 0.4678 0.4409 0.4912 0.4788 0.4698
  # 0.5067 0.4859 0.491  0.4786 0.4957 0.4961 0.4998 0.4815 0.502  0.4824
  # 0.4953 0.5127 0.4787 0.4866 0.4813 0.4942 0.5052 0.4896 0.4946 0.483 ]
 # [0.5038 0.4811 0.4927 0.4961 0.5215 0.4728 0.4778 0.513  0.4591 0.5106
  # 0.5225 0.4924 0.5117 0.481  0.4859 0.4992 0.4757 0.5076 0.4726 0.5131
  # 0.4778 0.4947 0.4962 0.4948 0.5121 0.4985 0.5123 0.5059 0.4948 0.4977]
 # [0.506  0.4922 0.5105 0.5068 0.5195 0.5003 0.5306 0.4946 0.5262 0.5058
  # 0.5179 0.5147 0.5229 0.525  0.5221 0.5108 0.5192 0.517  0.5057 0.5364
  # 0.5251 0.5136 0.5157 0.5139 0.5194 0.5001 0.4756 0.4947 0.5139 0.4971]
 # [0.5348 0.532  0.5482 0.551  0.5697 0.5427 0.5528 0.5431 0.5601 0.5566
  # 0.5516 0.5574 0.5555 0.5547 0.5543 0.5622 0.57   0.5539 0.5337 0.5502
  # 0.566  0.5585 0.5439 0.5505 0.5565 0.5663 0.5641 0.5594 0.55   0.5339]]
# list of pruned [[ 24.  28.  29.  29.  30.  29.  30.  30.  32.  31.  31.  32.  32.  32.
   # 31.  30.  31.  31.  31.  31.  31.  32.  32.  32.  32.  32.  32.  32.
   # 32.  32.]
 # [ 59.  58.  63.  64.  63.  67.  64.  67.  65.  65.  63.  63.  64.  65.
   # 64.  64.  65.  65.  66.  67.  65.  65.  65.  65.  65.  66.  67.  67.
   # 67.  67.]
 # [123. 126. 121. 128. 129. 134. 133. 131. 129. 130. 130. 130. 130. 128.
  # 129. 129. 128. 129. 132. 131. 132. 132. 132. 131. 130. 133. 133. 133.
  # 133. 133.]
 # [256. 267. 270. 262. 267. 271. 266. 266. 268. 268. 270. 268. 267. 270.
  # 270. 268. 269. 269. 269. 269. 270. 271. 271. 271. 271. 270. 270. 270.
  # 269. 269.]]
# list of rewards [[6.69588064 7.01411608 6.84853016 6.662246   6.7075234  6.88475208
  # 7.23274124 6.58204032 6.74245168 6.85887928 6.38152612 6.65060324
  # 6.5846276  6.74503896 6.52382652 6.51865196 6.47078728 6.7075234
  # 6.4423272  6.69588064 6.52900108 6.30390772 6.74374532 6.64154776
  # 6.71011068 6.54323112 6.40093072 6.60273856 6.53805656 6.6881188 ]
 # [8.93682632 9.34566541 9.13674323 9.07550742 8.61803989 9.49515283
  # 9.40510017 8.77112942 9.74189713 8.8143547  8.60002936 9.14214639
  # 8.79454311 9.34746647 9.25921485 9.01967477 9.44292229 8.8683863
  # 9.49875494 8.76932837 9.40510017 9.10072217 9.07370637 9.09892111
  # 8.7873389  9.03228214 8.78373679 8.8990042  9.09892111 9.04669057]
 # [8.92959949 9.17904984 8.84825699 8.9151386  8.68557198 9.03263333
  # 8.48492713 9.13566717 8.56446202 8.93321472 8.71449376 8.77233732
  # 8.62411319 8.58615336 8.63857408 8.84283415 8.69099481 8.73076226
  # 8.93502233 8.38008568 8.58434575 8.79222104 8.7542612  8.78679821
  # 8.68737959 9.03624856 9.47911331 9.13385956 8.78679821 9.09047689]
 # [9.66830808 9.72650082 9.38981425 9.33162151 8.94297714 9.50412142
  # 9.29421189 9.49580817 9.14249511 9.21523603 9.31915164 9.19860954
  # 9.23809747 9.25472396 9.26303721 9.09885055 8.93674221 9.27135046
  # 9.69116951 9.34824801 9.01987469 9.1757481  9.47918168 9.34201307
  # 9.21731435 9.01363976 9.05936262 9.15704329 9.35240463 9.68701289]]
  
  
# List of accuraccies [[0.3064 0.265  0.2642 0.3044 0.3281 0.3319 0.3132 0.2965 0.2746 0.2746
  # 0.2746 0.2746 0.2705 0.2705 0.3405 0.3394 0.3394 0.3394 0.3394 0.3394
  # 0.3475 0.3208 0.3491 0.3491 0.3491 0.3491 0.3491 0.3657 0.3657 0.3491]
 # [0.1109 0.102  0.1024 0.1129 0.1185 0.1159 0.1154 0.1096 0.1067 0.1049
  # 0.1052 0.1059 0.104  0.1042 0.1123 0.1123 0.1114 0.1144 0.1132 0.1124
  # 0.1116 0.1087 0.1169 0.1169 0.1226 0.1259 0.1259 0.1297 0.1303 0.1253]
 # [0.1098 0.1054 0.1048 0.1093 0.119  0.1179 0.1132 0.1097 0.1069 0.1053
  # 0.1053 0.1052 0.1043 0.1063 0.117  0.1167 0.1178 0.1213 0.1187 0.1178
  # 0.1163 0.1135 0.1243 0.1228 0.1307 0.1316 0.1315 0.1326 0.1389 0.134 ]
 # [0.1077 0.1058 0.1048 0.11   0.1272 0.127  0.1187 0.1213 0.1203 0.1163
  # 0.1175 0.1157 0.1139 0.1169 0.1444 0.1439 0.1427 0.1475 0.1423 0.131
  # 0.1228 0.1201 0.1337 0.1322 0.1413 0.1426 0.1391 0.1406 0.1548 0.1458]]
# list of pruned [[ 30.  30.  29.  30.  30.  29.  29.  29.  28.  28.  28.  28.  28.  28.
   # 31.  30.  30.  30.  30.  30.  31.  29.  31.  31.  31.  31.  31.  32.
   # 32.  31.]
 # [ 62.  60.  60.  63.  64.  63.  64.  65.  65.  64.  65.  67.  65.  65.
   # 64.  63.  62.  62.  61.  59.  59.  61.  60.  60.  63.  63.  64.  63.
   # 64.  65.]
 # [115. 115. 116. 117. 118. 119. 119. 119. 119. 117. 119. 119. 116. 117.
  # 117. 116. 116. 115. 115. 116. 115. 113. 114. 115. 116. 114. 116. 115.
  # 114. 113.]
 # [234. 223. 224. 228. 226. 229. 230. 229. 230. 232. 233. 231. 232. 230.
  # 229. 230. 230. 229. 229. 228. 229. 230. 231. 231. 233. 232. 232. 230.
  # 231. 232.]]
# list of rewards [[ 8.97268704  9.508254    9.51860312  8.99855984  8.69196716  8.64280884
   # 8.88471952  9.1007574   9.38406456  9.38406456  9.38406456  9.38406456
   # 9.4371038   9.4371038   8.5315558   8.54578584  8.54578584  8.54578584
   # 8.54578584  8.54578584  8.441001    8.78640288  8.42030276  8.42030276
   # 8.42030276  8.42030276  8.42030276  8.20555852  8.20555852  8.42030276]
 # [16.01316461 16.17345835 16.16625414 15.97714355 15.87628456 15.92311195
  # 15.93211722 16.03657831 16.08880885 16.12122781 16.11582465 16.10321728
  # 16.13743729 16.13383518 15.98794987 15.98794987 16.00415935 15.95012775
  # 15.97174039 15.98614881 16.00055724 16.05278778 15.90510142 15.90510142
  # 15.80244138 15.74300662 15.74300662 15.6745666  15.66376028 15.75381294]
 # [16.0913552  16.17089009 16.18173576 16.10039326 15.92505497 15.94493869
  # 16.02989642 16.09316281 16.14377593 16.1726977  16.1726977  16.17450532
  # 16.19077382 16.15462159 15.96120719 15.96663002 15.9467463  15.88347991
  # 15.9304778  15.9467463  15.97386047 16.02447358 15.82925157 15.85636574
  # 15.71356445 15.69729595 15.69910356 15.67921984 15.56534033 15.65391328]
 # [18.54477923 18.58426716 18.60505029 18.49697806 18.13950837 18.14366499
  # 18.3161649  18.26212878 18.2829119  18.36604439 18.34110464 18.37851426
  # 18.41592388 18.35357452 17.78203868 17.79243024 17.81736998 17.717611
  # 17.82568323 18.06053251 18.2309541  18.28706853 18.00441808 18.03559276
  # 17.84646635 17.8194483  17.89218922 17.86101454 17.56589422 17.75294231]]
# list of flops [[4.1515200e+05 4.1515200e+05 4.1515200e+05 4.1515200e+05 4.1515200e+05
  # 4.1515200e+05 4.1515200e+05 4.1515200e+05 4.1515200e+05 4.1515200e+05
  # 4.1515200e+05 4.1515200e+05 4.1515200e+05 4.1515200e+05 4.1515200e+05
  # 4.1515200e+05 4.1515200e+05 4.1515200e+05 4.1515200e+05 4.1515200e+05
  # 4.1515200e+05 4.1515200e+05 4.1515200e+05 4.1515200e+05 4.1515200e+05
  # 4.1515200e+05 4.1515200e+05 4.1515200e+05 4.1515200e+05 4.1515200e+05]
 # [6.6355200e+07 6.6355200e+07 6.6355200e+07 6.6355200e+07 6.6355200e+07
  # 6.6355200e+07 6.6355200e+07 6.6355200e+07 6.6355200e+07 6.6355200e+07
  # 6.6355200e+07 6.6355200e+07 6.6355200e+07 6.6355200e+07 6.6355200e+07
  # 6.6355200e+07 6.6355200e+07 6.6355200e+07 6.6355200e+07 6.6355200e+07
  # 6.6355200e+07 6.6355200e+07 6.6355200e+07 6.6355200e+07 6.6355200e+07
  # 6.6355200e+07 6.6355200e+07 6.6355200e+07 6.6355200e+07 6.6355200e+07]
 # [7.0852608e+07 7.0852608e+07 7.0852608e+07 7.0852608e+07 7.0852608e+07
  # 7.0852608e+07 7.0852608e+07 7.0852608e+07 7.0852608e+07 7.0852608e+07
  # 7.0852608e+07 7.0852608e+07 7.0852608e+07 7.0852608e+07 7.0852608e+07
  # 7.0852608e+07 7.0852608e+07 7.0852608e+07 7.0852608e+07 7.0852608e+07
  # 7.0852608e+07 7.0852608e+07 7.0852608e+07 7.0852608e+07 7.0852608e+07
  # 7.0852608e+07 7.0852608e+07 7.0852608e+07 7.0852608e+07 7.0852608e+07]
 # [1.0616832e+09 1.0616832e+09 1.0616832e+09 1.0616832e+09 1.0616832e+09
  # 1.0616832e+09 1.0616832e+09 1.0616832e+09 1.0616832e+09 1.0616832e+09
  # 1.0616832e+09 1.0616832e+09 1.0616832e+09 1.0616832e+09 1.0616832e+09
  # 1.0616832e+09 1.0616832e+09 1.0616832e+09 1.0616832e+09 1.0616832e+09
  # 1.0616832e+09 1.0616832e+09 1.0616832e+09 1.0616832e+09 1.0616832e+09
  # 1.0616832e+09 1.0616832e+09 1.0616832e+09 1.0616832e+09 1.0616832e+09]]
# list of valacc [0.5164 0.5647 0.5649 0.5929 0.6152 0.6112 0.605  0.63   0.6299 0.6511
 # 0.6569 0.6625 0.6525 0.6607 0.674  0.6645 0.6679 0.6747 0.6751 0.6801
 # 0.6906 0.6795 0.6766 0.6795 0.679  0.6934 0.6945 0.7061 0.691  0.7098
 # 0.7102 0.7053 0.715  0.7128 0.7125 0.7106 0.7016 0.7024 0.7171 0.7143
 # 0.7214 0.7102 0.7153 0.7185 0.7112 0.7123 0.6929 0.7232 0.7168 0.7294
 # 0.725  0.7219 0.7213 0.7194 0.724  0.7199 0.7254 0.7221 0.7254 0.7293
 # 0.7293 0.7276 0.7341 0.7334 0.7333 0.7351 0.7341 0.7332 0.7333 0.7422
 # 0.7325 0.7211 0.74   0.7238 0.7275 0.7372 0.7316 0.7352 0.7416 0.7396
 # 0.7306 0.7332 0.7397 0.7275 0.7401 0.7416 0.7332 0.736  0.7395 0.7422
 # 0.7246 0.7376 0.7388 0.7473 0.7441 0.7378 0.7459 0.     0.     0.    ]
