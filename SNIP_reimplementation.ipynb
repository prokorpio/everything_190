{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SNIP_reimplementation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prokorpio/everything_190/blob/master/SNIP_reimplementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0kkUsgZ25Ig",
        "colab_type": "text"
      },
      "source": [
        "# SNIP Reimplementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndqT7oLK21yS",
        "colab_type": "text"
      },
      "source": [
        "## 1. Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYdYODVYjxqG",
        "colab_type": "code",
        "outputId": "d8d04211-62c7-4dd7-f983-71e533897150",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "print('Tensorflow v', tf.__version__, sep='')\n",
        "from platform import python_version\n",
        "print('python v',python_version(), sep='')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense, Conv2D\n",
        "from tensorflow.keras.initializers import VarianceScaling\n",
        "from tensorflow.keras.models import Model, Sequential, load_model\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "import keras.backend as K\n",
        "import keras\n",
        "print('Keras v',keras.__version__,sep='')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow v1.15.0-rc3\n",
            "python v3.6.8\n",
            "Keras v2.2.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lixkD3g43Cr-",
        "colab_type": "text"
      },
      "source": [
        "## 2. Setup Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7nbgBIVlikd",
        "colab_type": "code",
        "outputId": "e0cecf63-5a55-474b-8981-3a4f32d5a33e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# load mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "img_size = x_train.shape[1:] # shape = [m, h, w]\n",
        "\n",
        "#from sklearn.model_selection import StratifiedKFold\n",
        "#folds = 7\n",
        "#cv = StratifiedKFold(n_splits=folds, random_state=42, shuffle=True)\n",
        "#x = np.concatenate((x, x_test))\n",
        "#y = np.concatenate((y, y_test))\n",
        "\n",
        "# normalize\n",
        "x_train = x_train.astype('float32')/255\n",
        "x_test = x_test.astype('float32')/255\n",
        "\n",
        "# split train:validation as 90%:10%\n",
        "#x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=0.1, shuffle=False)\n",
        "\n",
        "# convert y labels to one-hot vectors\n",
        "num_classes = len(np.unique(y_train))\n",
        "#y = to_categorical(y, num_classes) # do this inside cross-validation loop\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "# view sample shape\n",
        "print('xtrain shape:',x_train.shape)\n",
        "print('ytrain shape:',y_train.shape)\n",
        "print('xtest shape:',x_test.shape)\n",
        "print('ytest shape:',y_test.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xtrain shape: (60000, 28, 28)\n",
            "ytrain shape: (60000, 10)\n",
            "xtest shape: (10000, 28, 28)\n",
            "ytest shape: (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1IZjHhHCGJu",
        "colab_type": "text"
      },
      "source": [
        "## 3. Create Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO5jUk28XRpJ",
        "colab_type": "text"
      },
      "source": [
        "### 3.1 Define Custom Layers "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYQpEqbpXRHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# source: https://stackoverflow.com/questions/50290769/specify-connections-in-nn-in-keras\n",
        "\n",
        "class PrunableDense(Dense):\n",
        "\n",
        "    def __init__(self,units,mask,**kwargs):\n",
        "          \n",
        "        self.mask = mask         \n",
        "\n",
        "        #initalize the original Dense with all the usual arguments   \n",
        "        super(PrunableDense,self).__init__(units,**kwargs)  \n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        output = K.dot(inputs, self.kernel * self.mask)\n",
        "        if self.use_bias:\n",
        "            output = K.bias_add(output, self.bias)\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHNcXFBvXZ49",
        "colab_type": "text"
      },
      "source": [
        "### 3.2 Construct Custom Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Emqg7qU34dNs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LeNet_300_100(input_shape, num_classes,mask=[1,1,1]):\n",
        "    \"\"\" \n",
        "    LeNet 3-Layer FC implementation\n",
        "    \"\"\"\n",
        "    \n",
        "    vs = VarianceScaling()\n",
        "    \n",
        "    X_input = Input(shape = input_shape)\n",
        "    X = Flatten()(X_input) # 28 * 28 = 784\n",
        "    X = PrunableDense(300, \n",
        "                      mask = mask[0],\n",
        "                      use_bias = False,\n",
        "                      kernel_initializer = vs,\n",
        "                      activation='relu')(X) \n",
        "    X = PrunableDense(100,\n",
        "                      mask = mask[1],\n",
        "                      use_bias = False,\n",
        "                      kernel_initializer = vs,\n",
        "                      activation='relu')(X)\n",
        "    X = PrunableDense(num_classes, \n",
        "                      mask = mask[2],\n",
        "                      use_bias = False,\n",
        "                      kernel_initializer = vs,\n",
        "                      activation='softmax')(X)\n",
        "    \n",
        "    return Model(inputs=X_input, outputs=X)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl2UYsf7Q5s9",
        "colab_type": "text"
      },
      "source": [
        "## 4. Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEtX3BNt4haS",
        "colab_type": "code",
        "outputId": "bef6c6db-6b6b-415f-b549-3f27831c683b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# setup Hyperparams\n",
        "SGDdizer = SGD(lr=0.1,\n",
        "               momentum = 0.9,\n",
        "               decay = 0.0005)\n",
        "batch_size = 100\n",
        "epochs = 10\n",
        "    \n",
        "# fit model using Strat K-fold cross-validation\n",
        "scores = []\n",
        "trial = 0\n",
        "#for train_index, test_index in cv.split(x,y):\n",
        "for _ in range(5):\n",
        "    #x_train, y_train = x[train_index], y[train_index] \n",
        "    #x_test, y_test = x[test_index], y[test_index]\n",
        "    #y_train = to_categorical(y_train, num_classes) \n",
        "    #y_test = to_categorical(y_test, num_classes) \n",
        "    LeNet = LeNet_300_100(img_size, num_classes)\n",
        "    LeNet.compile(optimizer=SGDdizer,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "#    start_time = time.time()\n",
        "    LeNet.fit(x_train, y_train,\n",
        "              batch_size = batch_size,\n",
        "              epochs = epochs,\n",
        "              verbose=0, \n",
        "              validation_split=0.1)\n",
        "#    elapsed_time = time.time() - start_time\n",
        "#    print(\"Trained split\",split,end='')\n",
        "#    time.strftime(\" elapsed time = %H:%M:%S\", time.gmtime(elapsed_time))\n",
        "    preds = LeNet.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "    trial_error = 1-preds[1]\n",
        "    print(\"Error of trial\",trial,\"=\",trial_error)\n",
        "    scores.append(trial_error)\n",
        "    trial += 1\n",
        "print(\"Average Error of Original Network =\",np.mean(scores))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 0s 26us/sample - loss: 0.0653 - acc: 0.9831\n",
            "Error of split 0 = 0.01690000295639038\n",
            "10000/10000 [==============================] - 0s 27us/sample - loss: 0.0649 - acc: 0.9809\n",
            "Error of split 1 = 0.01910001039505005\n",
            "10000/10000 [==============================] - 0s 27us/sample - loss: 0.0695 - acc: 0.9790\n",
            "Error of split 2 = 0.021000027656555176\n",
            "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0730 - acc: 0.9782\n",
            "Error of split 3 = 0.021799981594085693\n",
            "10000/10000 [==============================] - 0s 27us/sample - loss: 0.0751 - acc: 0.9755\n",
            "Error of split 4 = 0.024500012397766113\n",
            "5-Fold Validation Error of Original Network = 0.020660006999969484\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhKpN-0tTxcv",
        "colab_type": "text"
      },
      "source": [
        "## 5. Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLPkeYsbM74j",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "def plot_trainings(trainings):\n",
        "    plt.clf()\n",
        "    acc = []\n",
        "    val_acc = []\n",
        "    loss = []\n",
        "    val_loss = []\n",
        "\n",
        "    # concatenate all data points for all trainings\n",
        "    for train in trainings:\n",
        "        for point in train.history['acc']:\n",
        "            acc.append(point)\n",
        "        for point in train.history['val_acc']:\n",
        "            val_acc.append(point)\n",
        "        for point in train.history['loss']:\n",
        "            loss.append(point)\n",
        "        for point in train.history['val_loss']:\n",
        "            val_loss.append(point)\n",
        "    #epochs = range(1, len(mae) + 1)\n",
        "\n",
        "    # Plot training history for accuracy\n",
        "    plt.plot(acc)\n",
        "    plt.plot(val_acc)\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gp6JQMvxUg1g",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "preds = LeNet.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "plot_trainings(trainings)\n",
        "print(\"\\nTest Error: {:.2%}\\n\".format(1-preds[1])) # should get 1.65ish error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM47sSHK5yMm",
        "colab_type": "text"
      },
      "source": [
        "# II. Perform SNIP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5Ye57FrfUYT",
        "colab_type": "code",
        "outputId": "dcf92661-a44b-417a-a145-8e1e5552d040",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# create initial mask matrix (all ones), using LeNet ^^\n",
        "mask = []\n",
        "for layer in LeNet.trainable_weights:\n",
        "    #print(layer.shape)\n",
        "    mask.append(tf.Variable(np.ones(layer.shape),\n",
        "                            trainable = False,\n",
        "                            dtype = 'float32'))\n",
        "for i, _ in enumerate(mask):\n",
        "    print('Mask for layer ', i+1,':',sep='')\n",
        "    print(mask[i])\n",
        "\n",
        "LeNet_to_Prune = LeNet_300_100(img_size, num_classes,mask=mask)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mask for layer 1:\n",
            "<tf.Variable 'Variable:0' shape=(784, 300) dtype=float32_ref>\n",
            "Mask for layer 2:\n",
            "<tf.Variable 'Variable_1:0' shape=(300, 100) dtype=float32_ref>\n",
            "Mask for layer 3:\n",
            "<tf.Variable 'Variable_2:0' shape=(100, 10) dtype=float32_ref>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8mbi9kO2Nio",
        "colab_type": "code",
        "outputId": "3c1e75e4-3ba3-4dcd-a771-9e0f17421075",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Compute Gradient wrt Mask\n",
        "labels = y_train[:100] # mini-batch of 100 on MNIST from Experiment Setup Section\n",
        "loss = K.mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=LeNet_to_Prune.outputs))\n",
        "grads = K.gradients(loss, mask) # get gradient of loss wrt mask\n",
        "\n",
        "trainingExample = x_train[:100]\n",
        "sess = tf.InteractiveSession()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "evaluated_grads = sess.run(grads,feed_dict={LeNet_to_Prune.input:trainingExample})\n",
        "sess.close()\n",
        "\n",
        "print('Grads shape per layer')\n",
        "for layer in evaluated_grads:\n",
        "    print(layer.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Grads shape per layer\n",
            "(784, 300)\n",
            "(300, 100)\n",
            "(100, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdlZo_HqG859",
        "colab_type": "code",
        "outputId": "8a8bc5f8-83b0-4c7b-9f3c-18f6aac16fcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# convert grads to a dictionary for matrix pruning and reconstruction\n",
        "# each weight should have an associated key\n",
        "model_layers = []\n",
        "for index, layer in enumerate(evaluated_grads):\n",
        "    print('Shape of layer',index+1, '=', layer.shape)\n",
        "    #print(type(layer))\n",
        "    model_layers.append({key:abs(grad) for key, grad in np.ndenumerate(layer)})\n",
        "                         # key of a weight is its position in tuple form\n",
        "                         # note, absolute value is already taken here\n",
        "    print('sample grad value:', model_layers[index][(99,9)]) # sanity check\n",
        "    print('vectorized? ', np.array(list(model_layers[index].values())).shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of layer 1 = (784, 300)\n",
            "sample grad value: 6.9194857e-07\n",
            "vectorized?  (235200,)\n",
            "Shape of layer 2 = (300, 100)\n",
            "sample grad value: 4.1439953e-06\n",
            "vectorized?  (30000,)\n",
            "Shape of layer 3 = (100, 10)\n",
            "sample grad value: 5.0648437e-06\n",
            "vectorized?  (1000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KpyM1dJdrPA",
        "colab_type": "code",
        "outputId": "a9fd0104-1feb-4093-d142-0bba2d588606",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# I. Normalize and get sensitivity per weight\n",
        "\n",
        "## 1. get sum of all grads\n",
        "sum_of_grads = 0\n",
        "for layer in model_layers: # layer = {(row, col) : grad_of_weight}\n",
        "    sum_in_layer = sum(layer.values())\n",
        "    #print(sum_in_layer)\n",
        "    sum_of_grads += sum_in_layer\n",
        "print('overall sum =',sum_of_grads)\n",
        "\n",
        "## 2. normalize each grad\n",
        "model_layers_normed = []\n",
        "for index, layer in enumerate(model_layers):\n",
        "    model_layers_normed.append({key:0 for key in layer.keys()})\n",
        "    for key in model_layers_normed[index].keys():\n",
        "        model_layers_normed[index][key] = layer[key]/sum_of_grads"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "overall sum = 0.7242147353031478\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNbL_xYkAm6O",
        "colab_type": "code",
        "outputId": "db7921b8-6637-4fe9-d59f-3d0ea1ba185c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# II. Get position of Top-K weights with highest sensitivity scores\n",
        "target_sparsity = 0.98 #(m-k)/m, where m is total params and k is non_zero w\n",
        "\n",
        "## x. Count total parameter values\n",
        "total_params = 0\n",
        "for i, layer_weights in enumerate(LeNet_to_Prune.trainable_weights):\n",
        "    total_params += int(np.prod(layer_weights.shape))\n",
        "print(\"# of total params =\",total_params)\n",
        "\n",
        "kappa = int(round(total_params * (1. - target_sparsity)))\n",
        "print(\"# of weights to keep =\", kappa)\n",
        "\n",
        "## x. SortDescending\n",
        "### concatenate the per layer dicts into 1 dict\n",
        "all_model_grads = {}\n",
        "for index, layer in enumerate(model_layers_normed):\n",
        "    all_model_grads.update({(index+1,) + key : grad for key, grad in layer.items()})\n",
        "                           # new key value = (layer,row,col)\n",
        "print('num of items =', len(all_model_grads))\n",
        "print('sample key:', list(all_model_grads.keys())[33] )\n",
        "print('sample value:', list(all_model_grads.values())[33])  \n",
        "print('check sum of all normed values = ',sum(all_model_grads.values()))\n",
        "\n",
        "### sort keys using values\n",
        "keys = list(all_model_grads.keys())\n",
        "values = list(all_model_grads.values())\n",
        "sorted_keys = sorted(all_model_grads.keys(), \n",
        "                     key=lambda k: all_model_grads[k],\n",
        "                     reverse = True)\n",
        "\n",
        "# get 1st k keys\n",
        "top_k_keys = sorted_keys[:kappa]\n",
        "\n",
        "# create mask of zeros\n",
        "pruning_mask = [] # list of np arrays\n",
        "for layer in evaluated_grads:\n",
        "    pruning_mask.append(np.zeros(layer.shape))\n",
        "    \n",
        "for i, _ in enumerate(pruning_mask):\n",
        "    print('Mask for layer ', i+1,':',sep='')\n",
        "    print(pruning_mask[i].shape)\n",
        "    \n",
        "# loop on the keys, and set mask value to 1\n",
        "for layer,row,col in top_k_keys:\n",
        "    pruning_mask[layer-1][row,col] = 1\n",
        "\n",
        "zero_norm = 0 # for sanity check\n",
        "for layer in pruning_mask:\n",
        "    zero_norm += np.sum(layer)\n",
        "print('zero norm =',zero_norm)\n",
        "print('kappa =', kappa)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of total params = 266200\n",
            "# of weights to keep = 5324\n",
            "num of items = 266200\n",
            "sample key: (1, 0, 33)\n",
            "sample value: 0.0\n",
            "check sum of all normed values =  1.000000000000016\n",
            "Mask for layer 1:\n",
            "(784, 300)\n",
            "Mask for layer 2:\n",
            "(300, 100)\n",
            "Mask for layer 3:\n",
            "(100, 10)\n",
            "zero norm = 5324.0\n",
            "kappa = 5324\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fp8C4II1wsPl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# more desired to just modify the mask of previous model's layers, \n",
        "#    instead of creating new model with new mask.\n",
        "# if not possible to modify the masks, then create new model and just load\n",
        "#    weights of the previous model. So the mask is the only difference \n",
        "#    (not the initialization etc)\n",
        "\n",
        "#Pruned_LeNet = LeNet_300_100(img_size, num_classes,mask=pruning_mask)\n",
        "#Pruned_LeNet.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "acd23043-b732-4abc-d57c-a8bc297a2ab8",
        "id": "C0axAzuEGpua",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# setup Hyperparams\n",
        "SGDdizer = SGD(lr=0.1,\n",
        "               momentum = 0.9,\n",
        "               decay = 0.0005)\n",
        "batch_size = 100\n",
        "epochs = 10\n",
        "    \n",
        "# fit model using Strat K-fold cross-validation\n",
        "scores = []\n",
        "trial = 0\n",
        "#for train_index, test_index in cv.split(x,y):\n",
        "for _ in range(5):\n",
        "    #x_train, y_train = x[train_index], y[train_index] \n",
        "    #x_test, y_test = x[test_index], y[test_index]\n",
        "    #y_train = to_categorical(y_train, num_classes) \n",
        "    #y_test = to_categorical(y_test, num_classes) \n",
        "    Pruned_LeNet = LeNet_300_100(img_size, num_classes, mask=pruning_mask)\n",
        "    Pruned_LeNet.compile(optimizer=SGDdizer,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "#    start_time = time.time()\n",
        "    Pruned_LeNet.fit(x_train, y_train,\n",
        "              batch_size = batch_size,\n",
        "              epochs = epochs,\n",
        "              verbose=0, \n",
        "              validation_split=0.1)\n",
        "#    elapsed_time = time.time() - start_time\n",
        "#    print(\"Trained split\",split,end='')\n",
        "#    time.strftime(\" elapsed time = %H:%M:%S\", time.gmtime(elapsed_time))\n",
        "    preds = Pruned_LeNet.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "    trial_error = 1-preds[1]\n",
        "    print(\"Error of trial\",trial,\"=\",trial_error)\n",
        "    scores.append(trial_error)\n",
        "    trial += 1\n",
        "print(\"Average Error of\", target_sparsity,\"Sparse Network =\",np.mean(scores))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 0s 29us/sample - loss: 0.1475 - acc: 0.9555\n",
            "Error of trial 0 = 0.044499993324279785\n",
            "10000/10000 [==============================] - 0s 31us/sample - loss: 0.2129 - acc: 0.9367\n",
            "Error of trial 1 = 0.06330001354217529\n",
            "10000/10000 [==============================] - 0s 30us/sample - loss: 0.2481 - acc: 0.9276\n",
            "Error of trial 2 = 0.07239997386932373\n",
            "10000/10000 [==============================] - 0s 29us/sample - loss: 0.2759 - acc: 0.9234\n",
            "Error of trial 3 = 0.07660001516342163\n",
            "10000/10000 [==============================] - 0s 29us/sample - loss: 0.2918 - acc: 0.9131\n",
            "Error of trial 4 = 0.08689999580383301\n",
            "Average Error of 0.98 Sparse Network = 0.06873999834060669\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_CCdAMc7BQc",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "# setup Hyperparams\n",
        "SGDdizer = SGD(lr=0.1,\n",
        "               momentum = 0.9,\n",
        "               decay = 0.0005)\n",
        "Pruned_LeNet.compile(optimizer=SGDdizer,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "batch_size = 100\n",
        "epochs = 10\n",
        "    \n",
        "# fit model\n",
        "trainings = []\n",
        "\n",
        "start_time = time.time()\n",
        "trainings.append(Pruned_LeNet.fit(\n",
        "                  x_train, y_train,\n",
        "                  batch_size = batch_size,\n",
        "                  epochs = epochs,\n",
        "                  validation_data = (x_valid, y_valid)))\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "time.strftime(\"Total training time  = %H:%M:%S\", time.gmtime(elapsed_time))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNGBqEB21PQ2",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "preds = Pruned_LeNet.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "plot_trainings(trainings)\n",
        "print(\"\\nTest Error: {:.2%}\\n\".format(1-preds[1])) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unXaSrxMyrbi",
        "colab_type": "text"
      },
      "source": [
        "# III. Experiment Results (For Pruning LeNet300-100)\n",
        "> From SNIP Paper\n",
        "+ Figure 1 in paper shows **start of increase in error @ sparsity = 90.**\n",
        "+ Table 1 shows **@ sparsity = 95% and 98%,** they got **1.6% and 2.4% errors** respectively. Their **reference error** (error from unpruned/original network) is **1.7%.**\n",
        "+ Errors from reimplementation were taken from the average of 5 evaluations.\n",
        "+ Note that from [Yann LeCun's implementation](http://yann.lecun.com/exdb/mnist/) himself, the error was 3.05%.\n",
        "\n",
        "\n",
        "Sparsity (%) | SNIP Error (%) | Reimplementation Error (%)\n",
        "--- | --- | ---\n",
        "orig | 1.7 | 2.07\n",
        "80 | ~1.73 | 2.52\n",
        "90 | ~1.78 | 3.26\n",
        "95 | 1.6 | 4.26\n",
        "98 | 2.4 | 6.87"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r394XkHW1d-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}